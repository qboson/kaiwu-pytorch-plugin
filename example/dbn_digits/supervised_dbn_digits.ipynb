{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd5ac77",
   "metadata": {},
   "source": [
    "### 堆叠受限玻尔兹曼机（RBM）原理与代码实现总结\n",
    "---\n",
    "#### **一、RBM原理概述**\n",
    "受限玻尔兹曼机（Restricted Boltzmann Machine）是一种基于能量的概率图模型，由可见层（Visible Layer）和隐层（Hidden Layer）组成，层内无连接，层间全连接。其核心是通过无监督学习学习数据的潜在特征分布。\n",
    "##### **1. 模型结构**\n",
    "- **可见层（v）**：输入数据的显式表示（如像素值）。\n",
    "- **隐层（h）**：提取的潜在特征。\n",
    "- **权重矩阵（W）**：连接可见层与隐层的权重。\n",
    "- **偏置**：可见层偏置（b）和隐层偏置（c）。\n",
    "##### **2. 能量函数与概率分布**\n",
    "RBM的能量函数定义为：\n",
    "$$\n",
    "E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^T W \\mathbf{h} - \\mathbf{b}^T \\mathbf{v} - \\mathbf{c}^T \\mathbf{h}\n",
    "$$\n",
    "联合概率分布通过玻尔兹曼分布给出：\n",
    "$$\n",
    "P(\\mathbf{v}, \\mathbf{h}) = \\frac{ e^{-E(\\mathbf{v}, \\mathbf{h})} }{Z}\n",
    "$$\n",
    "其中 $ Z $ 为配分函数（归一化因子）。可见层的边缘分布为：\n",
    "$$\n",
    "P(\\mathbf{v}) = \\sum_{\\mathbf{h}} P(\\mathbf{v}, \\mathbf{h})\n",
    "$$\n",
    "##### **3. 条件独立性**\n",
    "由于层内无连接，给定可见层时隐层条件独立，反之亦然：\n",
    "$$\n",
    "P(h_j=1|\\mathbf{v}) = \\sigma\\left(\\sum_i W_{ij} v_i + c_j\\right)\n",
    "$$\n",
    "$$\n",
    "P(v_i=1|\\mathbf{h}) = \\sigma\\left(\\sum_j W_{ij} h_j + b_i\\right)\n",
    "$$\n",
    "其中 $\\sigma(x) = \\frac{1}{1+e^{-x}}$ 为Sigmoid激活函数。\n",
    "##### **4. 训练目标**\n",
    "通过最大化似然函数学习参数（W, b, c）。目标函数为负对数似然：\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{\\mathbf{v}} \\log P(\\mathbf{v})\n",
    "$$\n",
    "采用对比散度（CD）算法近似梯度，更新规则为：\n",
    "$$\n",
    "\\Delta W_{ij} = \\epsilon \\left(\\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{recon}}\\right)\n",
    "$$\n",
    "其中 $\\epsilon$ 为学习率，$\\langle \\cdot \\rangle_{\\text{data}}$ 和 $\\langle \\cdot \\rangle_{\\text{recon}}$ 分别为数据分布和重构分布的期望。\n",
    "\n",
    "---\n",
    "#### **二、整体模块架构与训练模式**\n",
    "代码实现了基于PyTorch的深度信念网络(DBN)，采用分层架构设计，支持从无监督预训练到有监督学习的完整流程。\n",
    "\n",
    "- **模块架构**：\n",
    "  -  **DBNPretrainer**  \n",
    "      - 实现多层RBM的堆叠与逐层无监督预训练，提供特征提取接口。  \n",
    "  -  **AbstractSupervisedDBN**  \n",
    "      - 定义DBN监督学习的通用接口用于支持多模式训练策略，包括预训练、微调、分类器训练与预测等抽象方法。  \n",
    "  -  **AbstractSupervisedDBNClassifier**  \n",
    "      - 基于`AbstractSupervisedDBN`，实现PyTorch下的通用工具方法封装，包括特征提取、分类器集成(逻辑回归，支持向量机以及随机森林等)等。\n",
    "  -  **SupervisedDBNClassification**  \n",
    "      - 具体分类任务实现、微调网络构建以及训练。\n",
    "\n",
    "- **训练模式**：\n",
    "  1. 无监督模式: 仅预训练，用于特征提取\n",
    "  2. 分类器模式: 预训练 + 下游分类器训练\n",
    "  3. 微调网络模式: 预训练 + 网络反向传播微调\n",
    "   \n",
    "---\n",
    "#### **三、核心类功能和接口概述**\n",
    "\n",
    "##### **1. 核心类 `DBNPretrainer（无监督预训练DBN）`**\n",
    "- **关键参数**：\n",
    "  - `hidden_layers_structure`：隐层单元数（默认两层[100, 100]）\n",
    "  - `learning_rate_rbm`：RBM学习率（默认0.1）\n",
    "  - `n_epochs_rbm`：每层RBM训练轮数（默认10）\n",
    "  - `batch_size`：批大小（默认100）\n",
    "  - `verbose`：打印训练信息（默认True）\n",
    "  - `shuffle`：数据打乱（默认True）\n",
    "  - `drop_last`：是否丢弃最后不足batch的样本（默认False）\n",
    "  - `random_state`：随机种子\n",
    "- **设备支持**：自动选择GPU（`cuda`）或CPU\n",
    "- **核心方法**\n",
    "  - **创建RBM层 (`create_rbm_layer` 方法)**\n",
    "    - **初始化RBM**：使用 `RestrictedBoltzmannMachine` 定义可见层与隐层维度。\n",
    "  - **单批次训练步骤 (`_train_batch` 方法)**\n",
    "    1. **正相（Positive Phase）**：计算隐层激活概率 $ P(\\mathbf{h}|\\mathbf{v}) $。\n",
    "    2. **负相（Negative Phase）**：通过模拟退火采样器（`SimulatedAnnealingOptimizer`）生成重构样本。\n",
    "    3. **目标函数**：最小化能量函数加权重衰减（L2正则化）。\n",
    "    4. **反向传播**：更新权重和偏置。\n",
    "  - **单层RBM训练 (`_train_rbm_layer` 方法)**\n",
    "      - **初始化优化器**：采用随机梯度下降（SGD）优化参数。\n",
    "      - **DataLoader处理批量数据**\n",
    "  - **预训练堆叠RBM (`fit` 方法)**\n",
    "  - **特征变换，逐层提取特征 (`transform` 方法)**\n",
    "##### **2. 核心类 `AbstractSupervisedDBN（抽象接口定义）`**\n",
    "- **关键参数**：\n",
    "  - `fine_tuning`：模式选择（默认False）\n",
    "  - `learning_rate`：微调学习率（默认0.1）\n",
    "  - `n_iter_backprop`：反向传播迭代次数（默认100）\n",
    "  - `l2_regularization`：L2正则化（默认1e-4）\n",
    "  - `activation_function`：激活函数（默认'sigmoid'）\n",
    "  - `dropout_p`：Dropout概率（默认0.0）\n",
    "##### **3. 核心类 `AbstractSupervisedDBNClassifier（分类器模式具体实现，以及微调网络构建工具）`**\n",
    "- **关键参数**：\n",
    "  - `classifier_type`：支持多种分类器（默认逻辑回归）\n",
    "  - `clf_C`：正则化强度（默认1.0）\n",
    "  - `clf_iter`：迭代次数（默认100）\n",
    "##### **4. 核心类 `SupervisedDBNClassification（具体分类实现）`**\n",
    "- **微调网络构建**：使用预训练的权重来初始化(默认两层RBM，以及无Dropout层)\n",
    "  - 网络结构：输入层 → [线性层 + 激活函数 + Dropout] × N → 输出层\n",
    "  - 线性层：使用对应的RBM的权重初始化\n",
    "  - 输出层：随机初始化，在微调阶段学习\n",
    "\n",
    "- **训练策略**：\n",
    "  - 使用预训练权重初始化\n",
    "  - CrossEntropyLoss损失函数\n",
    "  - SGD优化器 + L2正则化\n",
    "  - 支持Dropout防止过拟合\n",
    "\n",
    "##### **5. 其他内容**\n",
    "\n",
    "- **数据加载 (`load_data` 方法)**\n",
    "\n",
    "    - 数据集：使用 `sklearn.datasets.load_digits`（8x8手写数字图像）。\n",
    "    - 增强：对原始图像进行上下左右平移，扩展数据集。\n",
    "\n",
    "- **训练过程可视化 (`_visualize_training_progress` 方法, 设置`plot_img=True`)**\n",
    "\n",
    "    - 权重与梯度：实时监控权重矩阵及其梯度变化。\n",
    "    - 生成样本：实时展示模型\"生成\"新样本的能力\n",
    "    - 重建样本：可视化重建误差的演变\n",
    "\n",
    "- **结果可视化 (`RBMVisualizer`类)**\n",
    "\n",
    "    - 训练后RBM权重可视化\n",
    "    - 分类任务结果: 混淆矩阵可视化\n",
    "    - 重建样本：训练完成后对测试图像进行编码-解码得到的重建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0e9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "seed = 42\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)       # 为当前GPU设置\n",
    "    torch.cuda.manual_seed_all(seed)   # 为所有GPU设置\n",
    "# Python\n",
    "random.seed(seed)\n",
    "# NumPy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bfdf9c-b6f5-41e8-8891-722220c7cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mode 1: Fine-tuning Network ===\n",
      "\n",
      "[DBN] Pre-training RBM layer 1/2: 64 -> 128\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.038658jmax 0.124560\n",
      "hmean 0.049939hmax 0.099375\n",
      "Iteration 1, Average Loss: 0.073776\n",
      "Iteration 6, Average Loss: 2.439374\n",
      "Iteration 11, Average Loss: 2.768574\n",
      "Iteration 16, Average Loss: 2.995442\n",
      "jmean 0.053279jmax 0.230121\n",
      "hmean 0.596039hmax 0.901621\n",
      "Iteration 21, Average Loss: 1.198484\n",
      "Iteration 26, Average Loss: 2.997356\n",
      "Iteration 31, Average Loss: 2.401676\n",
      "Iteration 36, Average Loss: 3.585153\n",
      "jmean 0.048890jmax 0.267820\n",
      "hmean 0.743854hmax 0.989778\n",
      "Iteration 41, Average Loss: 2.842974\n",
      "Iteration 46, Average Loss: 2.778290\n",
      "Iteration 51, Average Loss: 3.478456\n",
      "Iteration 56, Average Loss: 2.926089\n",
      "jmean 0.052351jmax 0.507418\n",
      "hmean 0.754552hmax 1.056937\n",
      "Iteration 61, Average Loss: 3.634552\n",
      "Iteration 66, Average Loss: 2.980318\n",
      "Iteration 71, Average Loss: 3.036174\n",
      "Layer 1, Epoch 1: Loss 2.833533\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: 2.833533\n",
      "[RBM] Layer 1, Epoch 1: Reconstruction Error = 0.104127\n",
      "\n",
      "jmean 0.048908jmax 0.596984\n",
      "hmean 0.807901hmax 1.020167\n",
      "Iteration 1, Average Loss: 0.463085\n",
      "Iteration 6, Average Loss: 0.793572\n",
      "Iteration 11, Average Loss: 1.597210\n",
      "Iteration 16, Average Loss: 2.734847\n",
      "jmean 0.047292jmax 0.682925\n",
      "hmean 0.726426hmax 0.996628\n",
      "Iteration 21, Average Loss: 3.949322\n",
      "Iteration 26, Average Loss: 4.955592\n",
      "Iteration 31, Average Loss: 6.015667\n",
      "Iteration 36, Average Loss: 7.450187\n",
      "jmean 0.055219jmax 0.773320\n",
      "hmean 0.740093hmax 1.016963\n",
      "Iteration 41, Average Loss: 7.974971\n",
      "Iteration 46, Average Loss: 8.324355\n",
      "Iteration 51, Average Loss: 9.337737\n",
      "Iteration 56, Average Loss: 10.130921\n",
      "jmean 0.063892jmax 0.897002\n",
      "hmean 0.769821hmax 1.047029\n",
      "Iteration 61, Average Loss: 10.384278\n",
      "Iteration 66, Average Loss: 10.677957\n",
      "Iteration 71, Average Loss: 10.326159\n",
      "Layer 1, Epoch 2: Loss 9.815123\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: 9.815123\n",
      "[RBM] Layer 1, Epoch 2: Reconstruction Error = 0.106950\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "\n",
      "[DBN] Pre-training RBM layer 2/2: 128 -> 256\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.038770jmax 0.128897\n",
      "hmean 0.052115hmax 0.097243\n",
      "Iteration 1, Average Loss: 0.205839\n",
      "Iteration 6, Average Loss: 6.935997\n",
      "Iteration 11, Average Loss: 4.269714\n",
      "Iteration 16, Average Loss: 6.969587\n",
      "jmean 0.050705jmax 0.618479\n",
      "hmean 0.666507hmax 1.081408\n",
      "Iteration 21, Average Loss: 3.307300\n",
      "Iteration 26, Average Loss: 0.753989\n",
      "Iteration 31, Average Loss: 0.556208\n",
      "Iteration 36, Average Loss: -1.901080\n",
      "jmean 0.057337jmax 1.076474\n",
      "hmean 0.903526hmax 1.074975\n",
      "Iteration 41, Average Loss: -6.067489\n",
      "Iteration 46, Average Loss: -12.400610\n",
      "Iteration 51, Average Loss: -17.910290\n",
      "Iteration 56, Average Loss: -26.507914\n",
      "jmean 0.067927jmax 1.094680\n",
      "hmean 0.948520hmax 1.053598\n",
      "Iteration 61, Average Loss: -31.977145\n",
      "Iteration 66, Average Loss: -38.897612\n",
      "Iteration 71, Average Loss: -39.279734\n",
      "Layer 2, Epoch 1: Loss -40.185732\n",
      "Output shape after layer 2: (7188, 128)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: -40.185732\n",
      "[RBM] Layer 2, Epoch 1: Reconstruction Error = 0.059437\n",
      "\n",
      "jmean 0.065763jmax 1.077016\n",
      "hmean 0.955249hmax 1.045057\n",
      "Iteration 1, Average Loss: -2.287209\n",
      "Iteration 6, Average Loss: -5.567828\n",
      "Iteration 11, Average Loss: -10.689078\n",
      "Iteration 16, Average Loss: -12.894223\n",
      "jmean 0.062654jmax 1.094772\n",
      "hmean 0.902128hmax 1.089504\n",
      "Iteration 21, Average Loss: -12.030533\n",
      "Iteration 26, Average Loss: -17.172434\n",
      "Iteration 31, Average Loss: -23.350993\n",
      "Iteration 36, Average Loss: -30.829142\n",
      "jmean 0.069732jmax 1.000249\n",
      "hmean 0.986476hmax 1.088955\n",
      "Iteration 41, Average Loss: -32.293148\n",
      "Iteration 46, Average Loss: -39.112076\n",
      "Iteration 51, Average Loss: -38.308906\n",
      "Iteration 56, Average Loss: -46.454097\n",
      "jmean 0.064714jmax 1.095055\n",
      "hmean 0.985723hmax 1.079012\n",
      "Iteration 61, Average Loss: -51.755510\n",
      "Iteration 66, Average Loss: -53.306645\n",
      "Iteration 71, Average Loss: -58.998000\n",
      "Layer 2, Epoch 2: Loss -61.336561\n",
      "Output shape after layer 2: (7188, 128)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: -61.336561\n",
      "[RBM] Layer 2, Epoch 2: Reconstruction Error = 0.059394\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "Starting fine-tuning...\n",
      "Built fine-tuning network with 5 layers\n",
      "Input size: 64\n",
      "Output size: 10\n",
      "Fine-tuning Epoch 10/100, Loss: 2.2719, Accuracy: 15.33%\n",
      "Fine-tuning Epoch 20/100, Loss: 1.8691, Accuracy: 33.95%\n",
      "Fine-tuning Epoch 30/100, Loss: 1.5092, Accuracy: 43.39%\n",
      "Fine-tuning Epoch 40/100, Loss: 1.3355, Accuracy: 50.78%\n",
      "Fine-tuning Epoch 50/100, Loss: 1.1590, Accuracy: 60.02%\n",
      "Fine-tuning Epoch 60/100, Loss: 0.9075, Accuracy: 69.82%\n",
      "Fine-tuning Epoch 70/100, Loss: 0.7251, Accuracy: 76.99%\n",
      "Fine-tuning Epoch 80/100, Loss: 0.5892, Accuracy: 81.76%\n",
      "Fine-tuning Epoch 90/100, Loss: 0.4882, Accuracy: 85.20%\n",
      "Fine-tuning Epoch 100/100, Loss: 0.4101, Accuracy: 87.55%\n",
      "Fine-tuning completed. \n",
      "Fine-tuning network training accuracy: 87.55%\n",
      "Fine-tuning network testing accuracy: 85.31%\n",
      "\n",
      "=== Trained Network Structure ===\n",
      "pretrain_layers: 2\n",
      "fine_tune_layers: 5\n",
      "hidden_units: [128, 256]\n",
      "num_classes: 10\n",
      "mode: fine_tuning\n",
      "Network structure saved to results/fine_tune_analysis_*\n",
      "\n",
      "=== Mode 2: Pipline Classifier ===\n",
      "\n",
      "[DBN] Pre-training RBM layer 1/1: 64 -> 128\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.043989jmax 0.125212\n",
      "hmean 0.051625hmax 0.097812\n",
      "Iteration 1, Average Loss: 0.071565\n",
      "Iteration 6, Average Loss: 1.972029\n",
      "Iteration 11, Average Loss: 1.771065\n",
      "Iteration 16, Average Loss: 2.552584\n",
      "jmean 0.049533jmax 0.199236\n",
      "hmean 0.433200hmax 0.772765\n",
      "Iteration 21, Average Loss: 3.893296\n",
      "Iteration 26, Average Loss: 3.657689\n",
      "Iteration 31, Average Loss: 3.354027\n",
      "Iteration 36, Average Loss: 3.168052\n",
      "jmean 0.057841jmax 0.221910\n",
      "hmean 0.717095hmax 1.069087\n",
      "Iteration 41, Average Loss: 2.989297\n",
      "Iteration 46, Average Loss: 1.497715\n",
      "Iteration 51, Average Loss: 0.539000\n",
      "Iteration 56, Average Loss: -1.025166\n",
      "jmean 0.053717jmax 0.539906\n",
      "hmean 0.848341hmax 1.089875\n",
      "Iteration 61, Average Loss: -0.469978\n",
      "Iteration 66, Average Loss: -0.249894\n",
      "Iteration 71, Average Loss: -2.728759\n",
      "Layer 1, Epoch 1: Loss -2.758270\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: -2.758270\n",
      "[RBM] Layer 1, Epoch 1: Reconstruction Error = 0.114207\n",
      "\n",
      "jmean 0.057355jmax 0.752302\n",
      "hmean 0.915198hmax 1.069050\n",
      "Iteration 1, Average Loss: -0.604369\n",
      "Iteration 6, Average Loss: -0.551907\n",
      "Iteration 11, Average Loss: -1.158491\n",
      "Iteration 16, Average Loss: -0.067009\n",
      "jmean 0.057246jmax 0.975949\n",
      "hmean 0.837358hmax 1.072500\n",
      "Iteration 21, Average Loss: 0.040118\n",
      "Iteration 26, Average Loss: -0.998128\n",
      "Iteration 31, Average Loss: -0.508391\n",
      "Iteration 36, Average Loss: -0.074709\n",
      "jmean 0.057313jmax 0.997141\n",
      "hmean 0.784035hmax 1.078888\n",
      "Iteration 41, Average Loss: 0.786670\n",
      "Iteration 46, Average Loss: 1.122143\n",
      "Iteration 51, Average Loss: 1.617993\n",
      "Iteration 56, Average Loss: 2.316063\n",
      "jmean 0.060109jmax 1.015459\n",
      "hmean 0.818605hmax 1.082622\n",
      "Iteration 61, Average Loss: 1.950001\n",
      "Iteration 66, Average Loss: 1.259735\n",
      "Iteration 71, Average Loss: 1.574832\n",
      "Layer 1, Epoch 2: Loss 1.991736\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: 1.991736\n",
      "[RBM] Layer 1, Epoch 2: Reconstruction Error = 0.129565\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "Training pipline classifier: logistic\n",
      "Classifier training accuracy: 84.95%\n",
      "Classifier testing accuracy: 83.42%\n",
      "\n",
      "=== Trained Network Structure ===\n",
      "pretrain_layers: 1\n",
      "fine_tune_layers: 0\n",
      "hidden_units: [128]\n",
      "num_classes: 10\n",
      "mode: classifier\n",
      "Network structure saved to results/classifier_analysis_*\n",
      "\n",
      "=== Feature Extraction ===\n",
      "Original shape: (1797, 64)\n",
      "Feature shape: (1797, 128)\n",
      "\n",
      "=== Save Parameters ===\n",
      "Pre-trained parameters saved for 2 layers\n",
      "Fine-tuned parameters saved for 3 layers\n",
      "All parameters saved successfully!\n",
      "Pre-trained parameters saved for 1 layers\n",
      "Classifier parameters saved to data/classifier_classifier.pkl\n",
      "All parameters saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from supervised_dbn_digits import load_data, RBMVisualizer, SupervisedDBNClassification\n",
    "\n",
    "def save_network_structure(dbn, filename_prefix):\n",
    "    \"\"\"保存网络结构信息到文件\"\"\"\n",
    "    structure = dbn.get_network_structure()\n",
    "    \n",
    "    # 保存为JSON\n",
    "    import json\n",
    "    with open(f\"results/{filename_prefix}_structure.json\", 'w') as f:\n",
    "        json.dump(structure, f, indent=2)\n",
    "    \n",
    "    # 保存为文本摘要\n",
    "    with open(f\"results/{filename_prefix}_summary.txt\", 'w') as f:\n",
    "        f.write(\"DBN Model Summary\\n\")\n",
    "        f.write(\"=================\\n\\n\")\n",
    "        f.write(f\"Pre-train layers: {structure['pretrain_layers']}\\n\")\n",
    "        f.write(f\"Fine-tune layers: {structure['fine_tune_layers']}\\n\")\n",
    "        f.write(f\"Hidden units: {structure['hidden_units']}\\n\")\n",
    "        f.write(f\"Number of classes: {structure['num_classes']}\\n\")\n",
    "        f.write(f\"Training mode: {structure['mode']}\\n\")\n",
    "    \n",
    "    print(f\"Network structure saved to results/{filename_prefix}_*\")\n",
    "\n",
    "def demonstrate_both_modes():\n",
    "    \"\"\"演示两种模式的使用\"\"\"\n",
    "    # 加载数据\n",
    "    X_train, X_test, y_train, y_test = load_data(plot_img=False)\n",
    "    \n",
    "    # 模式1: 使用微调网络\n",
    "    print(\"=== Mode 1: Fine-tuning Network ===\")\n",
    "    dbn_fine_tune = SupervisedDBNClassification(\n",
    "        hidden_layers_structure=[128, 256],\n",
    "        learning_rate_rbm=0.1,\n",
    "        n_epochs_rbm=2,\n",
    "        batch_size=64,\n",
    "        fine_tuning=True,  # 启用微调\n",
    "        learning_rate=0.1,\n",
    "        n_iter_backprop=100,\n",
    "        l2_regularization=1e-4,\n",
    "        activation_function='sigmoid',\n",
    "        verbose=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    dbn_fine_tune.fit(X_train, y_train)\n",
    "\n",
    "    # 评估模型\n",
    "    acc = dbn_fine_tune.score(X_test, y_test)*100\n",
    "    print(f\"Fine-tuning network testing accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # 获取训练后的网络结构信息\n",
    "    structure_info = dbn_fine_tune.get_network_structure()\n",
    "    print(\"\\n=== Trained Network Structure ===\")\n",
    "    for key, value in structure_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # 保存网络结构信息\n",
    "    save_network_structure(dbn_fine_tune, \"fine_tune_analysis\")\n",
    "    \n",
    "    # 模式2: 使用下游分类器\n",
    "    print(\"\\n=== Mode 2: Pipline Classifier ===\")\n",
    "    dbn_classifier = SupervisedDBNClassification(\n",
    "        hidden_layers_structure=[128],\n",
    "        learning_rate_rbm=0.1,\n",
    "        n_epochs_rbm=2,\n",
    "        batch_size=64,\n",
    "        fine_tuning=False,  # 使用分类器\n",
    "        # 选项: logistic, svm, random_forest\n",
    "        classifier_type = 'logistic', \n",
    "        clf_C=500,\n",
    "        clf_iter=1000,\n",
    "        verbose=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    dbn_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # 评估模型\n",
    "    acc2 = dbn_classifier.score(X_test, y_test)*100\n",
    "    print(f\"Classifier testing accuracy: {acc2:.2f}%\")\n",
    "\n",
    "    # 获取训练后的网络结构信息\n",
    "    structure_info = dbn_classifier.get_network_structure()\n",
    "    print(\"\\n=== Trained Network Structure ===\")\n",
    "    for key, value in structure_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # 保存网络结构信息\n",
    "    save_network_structure(dbn_classifier, \"classifier_analysis\") \n",
    "    \n",
    "    # 特征提取示例\n",
    "    print(\"\\n=== Feature Extraction ===\")\n",
    "    features = dbn_classifier.transform(X_test)\n",
    "    print(f\"Original shape: {X_test.shape}\")\n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    \n",
    "    return dbn_fine_tune, dbn_classifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model1, model2 = demonstrate_both_modes()\n",
    "    \n",
    "    # 保存模型参数\n",
    "    print(\"\\n=== Save Parameters ===\")\n",
    "    model1.save_parameters(file_prefix=\"fine_tune\")\n",
    "    model2.save_parameters(file_prefix=\"classifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21841a98-0736-4d75-9e68-20ad3f234398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fc9c2-f338-406d-b6ad-22e9b343ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
