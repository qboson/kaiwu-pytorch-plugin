{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 添加kaiwu的license信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIST数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTWithBatch(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, download=True, num_batches=6):\n",
    "        self.mnist = datasets.MNIST(root=root, train=train, transform=transform, download=download)\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_indices = self._create_batch_indices()\n",
    "\n",
    "    def _create_batch_indices(self):\n",
    "        num_samples = len(self.mnist)\n",
    "        return torch.arange(num_samples) // (num_samples // self.num_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.mnist[idx]  # 忽略原始 label\n",
    "        batch_idx = self.batch_indices[idx]\n",
    "        return image, batch_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarized MINIST数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据转换操作\n",
    "def flatten_tensor(x):\n",
    "    return x.view(-1)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为Tensor\n",
    "    transforms.Lambda(flatten_tensor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "epochs=20\n",
    "lr=1e-3\n",
    "kl_beta = 0.00001\n",
    "\n",
    "model_name = \"QVAE_annealing_tanh\"\n",
    "save_path = f\"./models/{model_name}\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MNISTWithBatch(root='../../data', train=False, download=False, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = MNISTWithBatch(root='../../data', train=True, download=False, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean_x = 0\n",
    "for x, _ in train_dataloader:\n",
    "    mean_x += x.mean(dim=0)\n",
    "mean_x = mean_x / len(train_dataloader)\n",
    "mean_x = mean_x.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MNISTWithBatch(root='../../data', train=False, download=False, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = MNISTWithBatch(root='../../data', train=True, download=False, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean_x = 0\n",
    "for x, _ in train_dataloader:\n",
    "    mean_x += x.mean(dim=0)\n",
    "mean_x = mean_x / len(train_dataloader)\n",
    "mean_x = mean_x.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qvae import QVAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 图片拉伸后的维度\n",
    "input_dim = 784\n",
    "# fc1压缩后的维度\n",
    "hidden_dim = 512\n",
    "# 隐变量维度\n",
    "latent_dim = 256\n",
    "\n",
    "# RBM可见层和隐藏层维度\n",
    "num_var1 = 128\n",
    "num_var2 = 128\n",
    "# 重叠分布的beta\n",
    "dist_beta = 10\n",
    "\n",
    "model = QVAE(input_dim, hidden_dim, latent_dim, num_var1, num_var2, dist_beta, mean_x)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # pip install kaiwu==1.0.26a --index-url http://__token__:glpat-7g5zHisFz1iK7Db77s_P@10.0.0.239:5005/api/v4/projects/114/packages/pypi/simple --trusted-host 10.0.0.239 --extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_counter = 0\n",
    "best_state_dict = None\n",
    "loss_history = []\n",
    "elbo_history = []\n",
    "kl_history = []\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs):\n",
    "    model.train()\n",
    "    total_loss, total_elbo, total_kl, total_cost = 0, 0, 0, 0\n",
    "    for x, _ in train_dataloader:\n",
    "        x = x.to(device)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, recon_x, neg_elbo, wd_loss, kl, cost, _, _ = model.neg_elbo(x, kl_beta)\n",
    "        loss = neg_elbo + wd_loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_elbo += neg_elbo.item()\n",
    "        total_kl += kl.item()\n",
    "        total_cost += cost.item()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    avg_elbo = total_elbo / len(train_dataloader)\n",
    "    avg_kl = total_kl / len(train_dataloader)\n",
    "    avg_cost = total_cost / len(train_dataloader)\n",
    "\n",
    "\n",
    "    loss_history.append(avg_loss)\n",
    "    elbo_history.append(avg_elbo)\n",
    "    kl_history.append(avg_kl)\n",
    "    cost_history.append(avg_cost)\n",
    "\n",
    "    model_save_path = os.path.join(save_path, f'davepp_epoch{epoch}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs}: Loss: {avg_loss:.4f}, elbo: {avg_elbo:.4f}, KL: {avg_kl:.4f}, Cost: {avg_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_txt(filename, data):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for value in data:\n",
    "            f.write(f\"{value:.6f}\\n\")\n",
    "\n",
    "\n",
    "save_list_to_txt(os.path.join(save_path, \"loss_history.txt\"), loss_history)\n",
    "save_list_to_txt(os.path.join(save_path, \"elbo_history.txt\"), elbo_history)\n",
    "save_list_to_txt(os.path.join(save_path, \"cost_history.txt\"), cost_history)\n",
    "save_list_to_txt(os.path.join(save_path, \"kl_history.txt\"), kl_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型加载模型文件\n",
    "# model.load_state_dict(torch.load(os.path.join(save_path, \"davepp_epoch10.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flattened_images_grid(features: torch.Tensor, grid_size: int = 8, save_path: str = None):\n",
    "    \"\"\"\n",
    "    显示并可选保存前 grid_size * grid_size 个 28x28 灰度图像。\n",
    "\n",
    "    Args:\n",
    "        features (torch.Tensor): 形状为 [N, 784] 的张量，每行为一个扁平的 28x28 图像。\n",
    "        grid_size (int): 图像网格边长（默认 8，即显示前 64 张图像）。\n",
    "        save_path (str): 如果提供，将保存图像到该路径。\n",
    "    \"\"\"\n",
    "    assert features.dim() == 2 and features.size(1) == 784, \"features 应为 [N, 784] 的张量\"\n",
    "    num_images = grid_size * grid_size\n",
    "    assert features.size(0) >= num_images, f\"features 中至少应包含 {num_images} 张图像\"\n",
    "\n",
    "    features_numpy = features[:num_images].detach().cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = i * grid_size + j\n",
    "            img = features_numpy[idx].reshape(28, 28)\n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        # os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        # plt.savefig(save_path, bbox_inches='tight')\n",
    "        # print(f\"图像已保存到 {save_path}\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flattened_images_grid(features, grid_size=8, save_path = save_path + f'/original.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# output, recon_x, neg_elbo, wd_loss, total_kl, cost = model.neg_elbo(features)\n",
    "\n",
    "output, recon_x, neg_elbo, wd_loss, kl, cost, q, zeta = model.neg_elbo(features, kl_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flattened_images_grid(output, grid_size=8, save_path = save_path + f'/recon_x.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaiwu.classical import SimulatedAnnealingOptimizer\n",
    "\n",
    "sampler = SimulatedAnnealingOptimizer(size_limit=100, alpha=0.99)\n",
    "z = model.rbm.sample(sampler)\n",
    "shape = z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Exponential\n",
    "\n",
    "smoothing_dist = Exponential(dist_beta)\n",
    "# 从平滑分布采样\n",
    "zeta = smoothing_dist.sample(shape)\n",
    "zeta = zeta.to(z.device)\n",
    "zeta = torch.where(z == 0., zeta, 0) # 引入z\n",
    "# zeta = torch.randn(256, 256).to(device)\n",
    "generated_x = model.decoder(zeta)\n",
    "generated_x = generated_x + model.train_bias\n",
    "\n",
    "generated_x = torch.sigmoid(generated_x)\n",
    "\n",
    "plot_flattened_images_grid(generated_x, grid_size=8, save_path = save_path + f'/generated_x.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "def get_real_images(dataloader, n_images=10000):\n",
    "    images = []\n",
    "    for batch_imgs, _ in dataloader:\n",
    "        images.append(batch_imgs)\n",
    "        if sum(img.shape[0] for img in images) >= n_images:\n",
    "            break\n",
    "    return torch.cat(images, dim=0)[:n_images]\n",
    "\n",
    "def generate_images_original_vae(model, latent_dim, n_images=10000, batch_size=64):\n",
    "    model.eval()\n",
    "    imgs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(n_images // batch_size)):\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            img = model.decoder(z).cpu()\n",
    "            imgs.append(img)\n",
    "    return torch.cat(imgs, dim=0)[:n_images]\n",
    "\n",
    "def generate_images_qvae(model, latent_dim, n_images=10000, batch_size=64):\n",
    "    model.eval()\n",
    "    imgs = []\n",
    "    sampler = SimulatedAnnealingOptimizer(alpha=0.95)\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(n_images // batch_size)):\n",
    "            z = model.rbm.sample(sampler)\n",
    "            # v, h = model.rbm.sample(256,10)\n",
    "            # z = torch.cat([v, h], dim=1)\n",
    "            shape = z.shape\n",
    "            smoothing_dist = Exponential(dist_beta)\n",
    "            # 从平滑分布采样\n",
    "            zeta = smoothing_dist.sample(shape)\n",
    "            zeta = zeta.to(z.device)\n",
    "            zeta = torch.where(z == 0., zeta, 0)\n",
    "            # zeta = torch.randn(256, 256).to(device)\n",
    "            generated_x = model.decoder(zeta)\n",
    "            \n",
    "            generated_x = generated_x + model.train_bias\n",
    "\n",
    "            generated_x = torch.sigmoid(generated_x)\n",
    "            \n",
    "            imgs.append(generated_x)\n",
    "    return torch.cat(imgs, dim=0)[:n_images]\n",
    "\n",
    "resize = transforms.Resize((299, 299))\n",
    "\n",
    "def preprocess(images):\n",
    "    if images.max() > 1.0:\n",
    "        images = images / 255.0\n",
    "    return resize(images)\n",
    "\n",
    "\n",
    "def compute_fid_in_batches(fake_imgs, real_imgs, batch_size=64):\n",
    "    \"\"\"\n",
    "    计算 FID 分数，适用于输入为 (N, 784) 的展平图像（如 MNIST）\n",
    "    \n",
    "    参数:\n",
    "        fake_imgs: 生成图像，shape = (N, 784)\n",
    "        real_imgs: 真实图像，shape = (M, 784)\n",
    "        batch_size: 每个批次处理多少图像\n",
    "        device: 使用 'cuda' 或 'cpu'\n",
    "    返回:\n",
    "        FID 分数\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=64).to(device)\n",
    "\n",
    "    def preprocess(images):\n",
    "        # 转换为图像格式 (B, 1, 28, 28)\n",
    "        images = images.view(-1, 1, 28, 28)\n",
    "        # 扩展为三通道\n",
    "        images = images.repeat(1, 3, 1, 1)\n",
    "        # 调整大小到 299x299\n",
    "        resize = transforms.Resize((299, 299), antialias=True)\n",
    "        return resize(images)\n",
    "\n",
    "    # 如果不是 tensor，先转成 tensor\n",
    "    if not isinstance(fake_imgs, torch.Tensor):\n",
    "        fake_imgs = torch.tensor(fake_imgs, dtype=torch.uint8)\n",
    "    if not isinstance(real_imgs, torch.Tensor):\n",
    "        real_imgs = torch.tensor(real_imgs, dtype=torch.uint8)\n",
    "\n",
    "    # 归一化到 [0, 255] 并转为 uint8（假定输入是 float 在 [0,1] 范围）\n",
    "    fake_imgs = (fake_imgs * 255).clamp(0, 255).to(torch.uint8)\n",
    "    real_imgs = (real_imgs * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "    # 转换为图像并更新 FID\n",
    "    for i in range(0, len(real_imgs), batch_size):\n",
    "        batch = real_imgs[i:i+batch_size]\n",
    "        batch = preprocess(batch)\n",
    "        fid.update(batch.to(device), real=True)\n",
    "\n",
    "    for i in range(0, len(fake_imgs), batch_size):\n",
    "        batch = fake_imgs[i:i+batch_size]\n",
    "        batch = preprocess(batch)\n",
    "        fid.update(batch.to(device), real=False)\n",
    "\n",
    "    return fid.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取真实图像和生成图像\n",
    "real_imgs = get_real_images(val_dataloader, n_images=10000)\n",
    "print(f\"Real images shape: {real_imgs.shape}\")\n",
    "\n",
    "fake_imgs_original_vae = generate_images_qvae(model, latent_dim=latent_dim)\n",
    "print(f\"Generated images shape: {fake_imgs_original_vae.shape}\")\n",
    "\n",
    "# 计算 FID（更节省内存）\n",
    "fid_original = compute_fid_in_batches(fake_imgs_original_vae, real_imgs)\n",
    "print(f\"Original VAE FID: {fid_original:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
